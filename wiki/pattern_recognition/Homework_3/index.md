---
layout: page
title: Homework 3
math_support: mathjax
---


## 1.1Consider a three-layer network for classification with nH nodes in hidden layer, and c nodes in output layer. The patterns (also say samples ) are in d dimensional space.  The activation function (or transfer function) for the nodes in the hidden layer is the sigmoid function. Differently, the nodes in the output layer will employ the following softmax operation as their activation function:$$z_j=\frac{e^{net_j}}{\sum_{m=1}^ce^{net_m}},j=1,2,\dots,c,$$where $net_j$ stands for the weighted sum at the $j$-th node in the output layer. Please derive the learning rule under the back propagation framework if the criterion function for each sample is the sum of the squared errors, that is （即分析每一层权重的更新方法）:$$J({\bf W})=\frac{1}{2}\sum_{j=1}^c(t_j-z_j)^2,$$where $t_j$ is the known target value for the sample at the $j$-th node in the output layer.**解**: 设输入层的一个节点输出为 $x_i, i=1,2,\dots,d$，设隐含层的一个节点输出为 $y_h, h=1,2,\dots,n_H$, 输出层的一个节点输出 $z_j, j=1,2,\dots,c$，并且，定义输入层到隐含层的权重矩阵为 ${\bf W}^{(1)}$，其中 $w^{(1)}_{ih} $ 对应着 $x_i$ 到 $y_h$ 的权重，输出层到隐含层的权重矩阵为 ${\bf W}^{2}$ 其中 $w^{(2)}_{hj}$ 对应 $y_h$ 到 $z_j$ 的权重.由题所述的损失函数和激活函数，暂时忽略在加权求和时的 bias 项，可得$$\begin{align}J({\bf W}) &= \frac{1}{2}\sum_{j=1}^c(t_j-z_j)^2 \\z_j &= \frac{\exp(net_j)}{\sum_{m=1}^c\exp(net_m)} \\net_j &= \sum_{h=1}^{n_H} w^{(2)}_{hj}y_h \\y_h &= \sigma(\sum_{i=1}^dw_{ih}^{(1)}x_i) \\\sigma(z) &= \frac{1}{1 + \exp(-z)}\end{align}$$方便起见，先计算两个函数的偏导：$$\begin{align}\frac{\partial z_j}{\partial net_k} &= \begin{cases}\z_j(1 - z_j) = z_j &- z_jz_j&, k = j \\&-z_j z_k &, k\ne j\end{cases} \\&= z_j I(k=j)-z_jz_k\\\frac{d\sigma(z)}{d z} &= z (1 - z) \end{align}$$同时，由于 $z_j$ 使用了 softmax，故每个输出节点的误差 $z_j - t_j$ 都会传播到其他节点。所以，对隐藏层到输出层权重的梯度为：$$\begin{align}\frac{\partial J}{\partial w^{(2)}_{hj}}&= \sum_{k=1}^c \frac{\partial J}{\partial z_k} \frac{\partial z_k}{\partial w^{(2)}_{hj}} \\&= \sum_{k=1}^c \frac{\partial J}{\partial z_k} (\sum_{m=1}^c \frac{\partial z_k}{\partial net_m} \frac{\partial net_m}{\partial w^{(2)}_{hj}})\end{align}$$又注意到，对输出节点 $j$ 无关的权重求导时均等于0，即有$$\frac{\partial net_m}{\partial w^{(2)}_{hj}} = \begin{cases}0 & m \ne j \\y_h & m = j\end{cases}$$所以梯度可以继续化简如下：$$\begin{align}\frac{\partial J}{\partial w^{(2)}_{hj}}&= \sum_{k=1}^c \frac{\partial J}{\partial z_k} \frac{\partial z_k}{\partial w^{(2)}_{hj}} \\&= \sum_{k=1}^c \frac{\partial J}{\partial z_k} \frac{\partial z_k}{\partial net_j} \frac{\partial net_j}{\partial w^{(2)}_{hj}} \\&= \frac{\partial J}{\partial z_j} \frac{\partial z_j}{\partial net_j} \frac{\partial net_j}{\partial w^{(2)}_{hj}} +\sum_{k \ne j} \frac{\partial J}{\partial z_k} \frac{\partial z_k}{\partial net_j} \frac{\partial net_j}{\partial w^{(2)}_{hj}} \\&= (z_j - t_j) z_j(1-z_j)y_h + \sum_{k \ne j}(z_k - t_k)(-z_kz_j)y_h \\&= ((z_j-t_j)z_jy_h - \sum_{k=1}^c(z_k - t_k)z_kz_jy_h\end{align}$$


