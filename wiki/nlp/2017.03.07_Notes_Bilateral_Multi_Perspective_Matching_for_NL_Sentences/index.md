---
layout: page
title: 2017.03.07 Notes Bilateral Multi Perspective Matching for NL Sentences
math_support: mathjax
---


**Bilateral Multi-Perspective Matching for Natural Language Sentences**Zhiguo Wang, Wael Hamza, Radu FlorianIBM T.J. Watson Research Center1101 Kitchawan Rd, Yorktown Heights, NY 10598 {zhigwang,whamza,raduf}@us.ibm.com## BackgroundSentence Matching- paraphrase classification- entailment recognition- Query-answer pair matching and candidate answer rankingTwo types of NN structures:- _Siamese_ architecture: one encoder for both sentence + matching, _no interaction between two sequences_- matching-aggregation (Wang and Jiang, 2016), matched firstly, then aggregated for final classificationProblems:- only word-by-word matching, no other granularity- only single direction matching> reason not that plausibleThus the new way:1. BiLSTM2. BiMatch, P to Q (p slice is matched with all q) and vice vesa3. BiLSTM4. FC Layer## TaskAn example is a triple$$(P, Q, y) = ( (p_1, p_2, \cdots, p_m), (q_1, q_2, \cdots, q_n), y)$$## Model![QQ20170306-220931.png](resources/34B3C651B54CBEDCD825BD6A37192FD8.png)- Word Embedding Glove concat Characters within a word LSTM- BiLSTM for context representation- Multi-pespective matching- BiLSTM for aggregation- 2 layer FFN### Multi-pespective Matchingdefine a multi-perspective matching function (different weighted cosine for all **perspectives**):$$\begin{align}m &= f_m( v_1,v_2; W) \\m_k &= \cos(W_k \circ v_1, W_k \circ v_2)\end{align}$$and concatenate all the four strategies:**Full Match**match a p slice with the final q, in both directions$$\overrightarrow m_i^{full} = f_m (\overrightarrow h_i^p, \overrightarrow h^q_N; W^1) \\\overleftarrow m_i^{full} = f_m (\overleftarrow h_i^p, \overleftarrow h^q_1; W^2)$$![QQ20170306-231724.png](resources/529D4383A4A49A6ABF0348770AB3A692.png)**Maxpooling Matching**match a p slice with all q slices, and use max pooling$$\overrightarrow m_i^{max} = \max_{j\in(1\cdots N)}f_m (\overrightarrow h_i^p, \overrightarrow h^q_j; W^3) \\\overleftarrow m_i^{max} = \max_{j\in(1\cdots N)}f_m (\overleftarrow h_i^p, \overleftarrow h^q_j; W^4)$$![QQ20170306-231731.png](resources/4BF71800234E8766C388970856826C02.png)**Attentive-Matching**match a p slice with an attentive q repr$$\begin{align}\overrightarrow\alpha_{i,j} &= cosine(\overrightarrow h_i^p, \overrightarrow h_j^q), j = 1, \dots, N \\\overleftarrow\alpha_{i,j} &= cosine(\overleftarrow h_i^p, \overleftarrow h_j^q), j = 1, \dots, N \\\overrightarrow h_i^{mean} &= \frac{\sum_j \overrightarrow\alpha_{i,j}\overrightarrow h_j^q}  {\sum_j \overrightarrow\alpha_{i,j}} \\\overleftarrow h_i^{mean} &= \frac{\sum_j \overleftarrow\alpha_{i,j}\overleftarrow h_j^q}  {\sum_j \overleftarrow\alpha_{i,j}} \\\overrightarrow m_i^{att} &= f_m(\overrightarrow h_i^p, \overrightarrow h_i^{mean}; W^5) \\\overleftarrow m_i^{att} &= f_m(\overleftarrow h_i^p, \overleftarrow h_i^{mean}; W^5) \\\end{align}$$![QQ20170306-231735.png](resources/B481991FDC5A8C05DD07F7880631A476.png)**Max-Attentive-Matching**match a p slice with a max-pooling over attentive q slices, instead of weighted sum![QQ20170306-231743.png](resources/384F3C126197AC837543E94D791EC9CA.png)## Experimentssettings:- char embedding is 20-d, word is composed to 50d by LSTM- all BiLSTM uses 100d- perspectives = 20- dropout ratio = 0.1, learning rate = 0.001- word embedding not updatedmulti-pespective(on paraphrase): l = 1, 5, 10, 15, 20![QQ20170306-232521.png](resources/495314BCA15665A0B6326D71BDECD9F8.png)bi-direction and four matching strategies(on paraphrase):![QQ20170306-232613.png](resources/43ACC2C60C115DBCD559BC52C52F0E4D.png)Paraphrase (on Quora Question Pairs (400k)):![QQ20170306-233553.png](resources/49CCC4DD6272DF4942B5F0AAD125FDF4.png)RTE(SNLI):![QQ20170306-233617.png](resources/D50241B71D968F75DB27D83AA13EF72B.png)Answer Selection(WikiQA):![QQ20170306-233715.png](resources/C49B77603BF7EDEBF3FE505F0FFA56DF.png)


