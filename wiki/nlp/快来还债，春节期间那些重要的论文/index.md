---
layout: page
title: 快来还债，春节期间那些重要的论文
math_support: mathjax
---


http://mp.weixin.qq.com/s?__biz=MzAwMjM3MTc5OA==&mid=404476249&idx=1&sn=8c1449057d8af89cf0a0cfea8169abdf&scene=0#wechat_redirect

**今天涉及的论文有：**

\1. 《BinaryNet: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1》

\2. 《Binaryconnect: Training deep neural networks with binary weights during propagations》

\3. 《Swivel: Improving Embeddings by Noticing What's Missing》

\4. 《Random Walks on Context Spaces: Towards an explanation of the mysteries of semantic word embeddings

》

\5. 《Associative Long Short-Term Memory》

\6. 《How to Train Deep Variational Autoencoders and Probabilistic Ladder

Networks》

\7. 《A Note on the Evaluation of Generative Models》

\8. 《Exploiting Cyclic Symmetry in CNNs》

\9. 《Spatial Transformer Networks》

BinaryNet: +1 or -1

这篇论文[1]可以说算是春节期间引起最热烈讨论的论文之一了。论文非常简短精炼，有兴趣的同学可以直接去读原文。这里我来简单概括下论文的贡献。题目已经说明一切，作者们提出在 DNN 中，**我们实际上可以把 weight 和 activation 都进行二值化（binarize）而提高 GPU 运算速度**，使得 training speed 提高7倍以上，并且结果几乎不受影响。具体地，在前向传递过程中，BinaryNet 首先把 weight 和 activation 都二值化后，然后把这些 Binary Matrices 进行了 convolve/dot product，之后再进行了 Batch Normalization（BN）。注意，这里的 BN 后的 parameter 就不再是 binary 的了。因为 BN 的 parameter 并不多，所以不再进行 binarize 也没什么。相反，作者指出，不进行 BN 的话，BinaryNet 的 performance 会有很大下降。关于这点，最近的许多工作中，对于 BN 的重要性也给出了同样的结论。

另外值得一提的是，这篇论文与之前一篇 BinaryConnect[2] 的论文很相似，但是其主要区别有：BinaryConnect 只二值化了 weights，而 BinaryNet 则是 weights + activations 都变成了 binary，这样充分利用了基于 GPU 的 XNOR 和 Popcount 的计算效率，最终使得前向过程被大幅提速。另外，这篇文章还提出了一些其他的 GPU kernel。关于作者们是如何改进 GPU kernel 的，用到的 SIMD 方法等等，可以见原论文 page 6 的左下角，这里就不赘述了。

但是除了这些，这篇论文背后给出的一些 **intuitions** 却更值得思考。第一，个人认为，把 weights 和 activations 都二值化，类似于去模拟人类的思考过程，但是比起决策树这种经典又 powerful 的模型，基于 DNN 的决策网络则富有表现力，更 representative，因为其不再只拥有一个父结点。第二点，二值化后的 weights 和 activations 都是 1-bit precision 的，大大提高了 GPU 计算效率，与此同时节省了计算空间。而实际上，无论是过去的实验还是现在的改进都发现，DNN 中大部分的 node 是处于 inactive 或者 无足轻重的地位。那么，关于这些 node 的计算如何被节省下来？因为毕竟，虽然 BinaryNet 中的 weights 和 activations 是二值化的，但是 gradients 还是多比特的。

Swivel: Improving Embeddings

这篇论文[3]的本质还是在基于共现的 word embedding 上做细节改进，并且大部分都是 Implementation details。不过，既然是 Google 出品，还是引起了不小的关注。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyBpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBXaW5kb3dzIiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOkJDQzA1MTVGNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOkJDQzA1MTYwNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6QkNDMDUxNUQ2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6QkNDMDUxNUU2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz6p+a6fAAAAD0lEQVR42mJ89/Y1QIABAAWXAsgVS/hWAAAAAElFTkSuQmCC)

这篇论文中提出的 Swivel，不仅在相关 task 上都超越了 CBOW, GloVe，SGNS 模型，**并且其性能与 corpus 大小无关**，在一定程度上有更好的扩展性。而其最大的贡献，就是改进了一种过去（几乎）从没被考虑过的情况：当两个词（当前词和上下文词）之间，没有共现数据时，是真实语言空间就不存在共现（两个词之间毫无瓜葛）还是语料不足导致的“假无关联”？为了处理这两种不同的情况，Google 将 loss function 改成了 piecewise 的，并且提出了不同的 penalty。只不过，这个 penalty 看起来真是 full of magic，无法理解为什么用这样奇怪的方式做“smoothing”，更无法理解为什么 f(x) 是这么一个很奇特的表达式。只不过，这让我想起了另外一篇论文，《Random Walks on Context Spaces》[4]，将 GloVe，Skip-Gram 等论文中的一些曾经的 trick 给出了一定程度上的数学解释。也就是说，就像 Google 自己说的，尽管他们这个 Swivel 看起来很 adhoc，但是也许有一天可以被科学证明。

另一方面，Swivel 在 implementation 上的改进则主要集中于如何处理巨大共现矩阵，如何分片，如何分布，如何通信。但是，这些改进与具体的 learning procedure 之前的关系并没有被很好地表现出来，恐怕这也不是 Google 所关心的。

Associative LSTM

这篇论文的核心贡献是提供了另一种扩展 recurrent neural networks with large external data 的方法——这不禁让人想到 Neural Turing Machines（NTM）。与此同时，过去的 LSTM 的一大问题就是无法很好地对数据进行索引，所以也就有了 attention mechansim。

针对这两个问题，Google 在这篇论文[5] 中提出了一种基于 **Holographic Reduced Representation（HRR）**的机制改进了 LSTM，并在 copy，arithmetic，sequence prediction in NLP 等任务上取得了不俗的表现。并且，比起 NTM，他们基于 HRR 的地址机制减少了写操作时的地址查询。更重要的是，他们的这个 associative LSTM 并不增加模型参数，却拥有了更好的 capacity。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyBpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBXaW5kb3dzIiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOkJDQzA1MTVGNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOkJDQzA1MTYwNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6QkNDMDUxNUQ2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6QkNDMDUxNUU2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz6p+a6fAAAAD0lEQVR42mJ89/Y1QIABAAWXAsgVS/hWAAAAAElFTkSuQmCC)

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyBpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBXaW5kb3dzIiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOkJDQzA1MTVGNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOkJDQzA1MTYwNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6QkNDMDUxNUQ2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6QkNDMDUxNUU2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz6p+a6fAAAAD0lEQVR42mJ89/Y1QIABAAWXAsgVS/hWAAAAAElFTkSuQmCC)
那么，究竟什么是 HRR 呢。HRR 可以讲 key-value 的 pair array data 压缩成一个 fixed-size vector——如果每一个 pair 是一个 fixed-size vector，那么最终 HRR 表达后的 array 就是同样大小的加和 vector，这个 array 就叫做 **memory trace**。原始的 HRR 压缩后的表达，再进行 retrieve 会带来一定的损失——但是经过这篇论文[5]的改进，多增加 copy storage，就可以减少损失，从而提高 retrieve 的能力——于是乎，最终，这篇论文是**将 LSTM + Redundant Associate Memory 结合**，并取得了很好的实验结果。

Deep VAE

这篇论文[6]之所以推荐并不是因为其理论新颖或者实验效果斐然，而是其在实验分析中使用的分析方法，对于对 how to evaluate generative models 感兴趣的同学会非常有启发。

先说这篇文章大概做了什么，Auto-Encoder（AE）想必大家已经不陌生，利用 reconstruction error 去学习一个 latent representation。然而，基于 AE 框架的模型很容易过度记忆数据，而并非学习数据背后真实的样子。为此，Variational AE（VAE）被提出来了，其理念在于假定建模一个数据背后的表达，并让最终学到的表达满足一定的分布性质。但是，VAE 的学习过程中，information flow 并不十分流畅，较为复杂，导致了过去的基于 VAE 框架的 generative model 都只能 train 2-3层。**这篇论文便是改进了 VAE 框架中的 inference 和 generative flow，使得 training 过程更加有效**。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyBpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBXaW5kb3dzIiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOkJDQzA1MTVGNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOkJDQzA1MTYwNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6QkNDMDUxNUQ2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6QkNDMDUxNUU2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz6p+a6fAAAAD0lEQVR42mJ89/Y1QIABAAWXAsgVS/hWAAAAAElFTkSuQmCC)

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyBpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBXaW5kb3dzIiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOkJDQzA1MTVGNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOkJDQzA1MTYwNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6QkNDMDUxNUQ2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6QkNDMDUxNUU2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz6p+a6fAAAAD0lEQVR42mJ89/Y1QIABAAWXAsgVS/hWAAAAAElFTkSuQmCC)
除了改进 information flow，这篇论文[6]还提出了一种 warm-up learning scheme，大概就是说在 training 一开始，并不直接进行 full training objective，而是将 objective 中的 regularization term 先忽略掉。这样的 scheme 个人理解有一些像 Highway Networks 初始化时尽量选择 gate 关闭一样——是基于训练得到的 NN 中的 node 激活程度的观察而得出的经验。

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyBpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBXaW5kb3dzIiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOkJDQzA1MTVGNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOkJDQzA1MTYwNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6QkNDMDUxNUQ2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6QkNDMDUxNUU2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz6p+a6fAAAAD0lEQVR42mJ89/Y1QIABAAWXAsgVS/hWAAAAAElFTkSuQmCC)

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAIAAACQd1PeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyBpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBXaW5kb3dzIiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOkJDQzA1MTVGNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOkJDQzA1MTYwNkE2MjExRTRBRjEzODVCM0Q0NEVFMjFBIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6QkNDMDUxNUQ2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6QkNDMDUxNUU2QTYyMTFFNEFGMTM4NUIzRDQ0RUUyMUEiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz6p+a6fAAAAD0lEQVR42mJ89/Y1QIABAAWXAsgVS/hWAAAAAElFTkSuQmCC)
回到正题，之所以推荐这篇论文是因为个人很喜欢他们在实验最后部分的分析方式，将 generation 过程中，不同阶段得到的 distribution 结果，latent representations 结果，还有其他因素可能的影响都考虑了进去。如果想对 **evaluation for generative models** (especially for image) 进行了解，我同时还推荐论文[7]。这篇论文虽然看起来简单，但是给出了非常重要的几点思考。而论文[6] 则是给出了理论上可能的启发。

Cyclic Symmetry in CNNs

曾经，有一句话，要想学出一个好的 CNN，就需要大数据！然而，这句话在 Data Augmentation 面前黯然失色。Data Augmentation 是指，在不改变 training data 的 label 的前提下，对 data 进行一定程度的 transform。对于图像来说，常见的 transform 方式有 crop/scale/rotate（对于文本来说，Data Augmentation 的方式较少，比较经典的可以参考《Teaching Machines to Read and Comprehend》中的相应部分）。使用 Data Augmentation 的目的，无非是学出一个更 generalized 的 CNN，而使用 Data Augmentation 的基础在于图像中的许多 feature 具有平移/旋转不变性。

然而，即使是 Data Augmentation，也不能保证绝对的 generalized。与此同时，不经过很好地设计的 Data Augmentation，虽然 implementation 很容易（所以请多用），却很占用 computation cost。为此，这篇论文[8] 提出了一种专门的 design 方式，将四种（甚至八种）图像的 rotation 方式，“嵌入”成 NN 的 layer ，**并最终在实验中达到了单单用 Data Augmentation 无法达到的好效果**。

![](http://mmbiz.qpic.cn/mmbiz/qrpuZA2scNrShIotiakHvDTJ8qFarwF0t7I8XYwad3foFzZNuEfBSgxQ22S9yAEezicMZMHAibxVNX2A42zJbGthA/640?wx_fmt=png&wxfrom=5&wx_lazy=1)

![](http://mmbiz.qpic.cn/mmbiz/qrpuZA2scNrShIotiakHvDTJ8qFarwF0t7I8XYwad3foFzZNuEfBSgxQ22S9yAEezicMZMHAibxVNX2A42zJbGthA/640?wx_fmt=png&wxfrom=5&wx_lazy=1)
这篇论文[8]中最基本的四个 layer 分别是 slicing，pooling，stacking 和 rolling，都非常简便易实现。但是在具体实现过程中，虽然既可以用改变 feature map 的方式，也可以用改变 filter 的方式来达到同样的效果，这篇论文却采用了前者。这里的原因主要是实现起来的方便，改变 feature map 的方式可以使得这些 layer 变成一个可任意插入 NN 的独立的 component，就像 ResNet 中的结构一样；当然，既然采用了 feature map 实现这四种操作的方式，就需要保证 feature map 都是 square 的。所以，最终的 CNN 中，如果要加入这四种 layer，只需要改变 minibatch setting。

总结起来，个人认为这篇论文最值得推荐的原因无非是提供了一种更灵活的“超” Data Augmentation（我瞎起的名字）的方法。就像 Data Augmentation 一样，它简单有效，甚至足够 flexible 到可以让你随便 creative。但是，它也过于简单。相比之下，NIPS'15 中的另一篇论文，《Spatial Transformer Networks》[9] 也是可以独立作为 component 加入 NN 中任意地方的，它的设计更为复杂（相对），但可实现的效果（任意角度的旋转等等）则更丰富。




